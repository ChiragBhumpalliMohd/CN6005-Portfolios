# ================================================================
# ðŸ§  Artificial Neural Network for Diabetes Prediction (Optimized)
# Using SMOTE for data balancing
# ================================================================

# --- Step 0: Import libraries ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam
from google.colab import files
import io

# --- Step 1: Upload and load dataset ---
print("ðŸ“‚ Please upload your Diabetes_ANN.csv file...")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Try multiple encodings to ensure compatibility
encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']
loaded = False
for enc in encodings:
    try:
        data = pd.read_csv(io.BytesIO(uploaded[file_name]), encoding=enc)
        print(f"âœ… File loaded successfully using encoding: {enc}")
        loaded = True
        break
    except Exception as e:
        print(f"âŒ Failed with encoding {enc}: {e}")

if not loaded:
    raise ValueError("âš ï¸ Could not load file â€” please make sure itâ€™s a valid CSV file exported from Numbers or Excel.")

print("\nðŸ“Š Dataset Info:")
print("Columns:", list(data.columns))
print(data.head())

# --- Step 2: Prepare data ---
if 'Outcome' not in data.columns:
    raise KeyError("Column 'Outcome' not found in dataset. Please ensure your CSV has it.")

X = data.drop(columns=['Outcome'])
y = data['Outcome']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Step 3: Apply SMOTE for class balancing ---
print("\nâš–ï¸ Applying SMOTE for class balance...")
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
print("Before SMOTE:", np.bincount(y_train))
print("After SMOTE:", np.bincount(y_train_res))

# --- Step 4: Standardize features ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

# --- Step 5: Build ANN model ---
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],),
          kernel_regularizer=regularizers.l2(0.0005)),
    BatchNormalization(),
    Dropout(0.2),

    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),
    BatchNormalization(),
    Dropout(0.2),

    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.0003),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# --- Step 6: Callbacks ---
early_stop = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)

# --- Step 7: Train model ---
print("\nðŸš€ Training model...")
history = model.fit(
    X_train_scaled, y_train_res,
    epochs=200,
    batch_size=16,
    validation_split=0.2,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# --- Step 8: Plot Accuracy and Loss ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()

# --- Step 9: Evaluate model ---
y_pred_prob = model.predict(X_test_scaled)
y_pred = (y_pred_prob > 0.5).astype(int)  # threshold fixed at 0.5

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Diabetes', 'Diabetes'],
            yticklabels=['No Diabetes', 'Diabetes'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# --- Step 10: Print metrics ---
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"\nâœ… Final Model Performance:")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# --- Step 11: Save model ---
model.save("diabetes_ann.h5")
print("\nðŸ’¾ Model saved as 'diabetes_ann.h5'. You can download it using:")
print("files.download('diabetes_ann.h5')")

